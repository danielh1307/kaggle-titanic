{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Solving Titanic Kaggle Challenge with a Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wandb login APIKEY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score\n",
    "\n",
    "import wandb\n",
    "import argparse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choose a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we define our model\n",
    "# It is a simple feedforward neural network\n",
    "\n",
    "# Input Layer --> number of features from our dataset\n",
    "# Hidden Layers\n",
    "# Activation Function: ReLU (Rectified Linear Unit) --> this is for introducing non-linearity\n",
    "# Output Layer: 1 neuron with a sigmoid activation function, representing the survival probability\n",
    "class TitanicNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TitanicNN, self).__init__()\n",
    "\n",
    "        # This is the first hidden layer with 128 neurons\n",
    "        # It takes the input features and applies a linear transformation followed by the ReLU activation function\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "\n",
    "        # Second hidden layer with 64 neurons. It takes the output of the first hidden layer and applies another\n",
    "        # linear transformation followed by the ReLU activation function\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # Output layer with 1 neuron. It takes the output of the second hidden layer and applies a linear transformation\n",
    "        # followed by the sigmoid activation function.\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "#device = \"cpu\" # uncomment if you want to use \"cpu\", currently cpu is faster than cuda (maybe because the NN is very little)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "number_of_featuers = 5\n",
    "model = TitanicNN(number_of_featuers).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choose a loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For binary classification tasks, the most common choice of loss function is the binary cross-entropy loss (log loss)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loads just the training data\n",
    "# returns two DataFrames, one with just the features and one with the labels\n",
    "def load_titanic_train_preprocessed():\n",
    "    train_preprocessed = pd.read_csv(os.path.join('data', 'train_preprocessed.csv'))\n",
    "    train_preprocessed_features = train_preprocessed.drop('Survived', axis=1)\n",
    "    train_preprocessed_label = train_preprocessed['Survived']\n",
    "    return train_preprocessed_features, train_preprocessed_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loads just the test data\n",
    "# returns two DataFrames, one with just the features and one with the labels\n",
    "def load_titanic_train_test_preprocessed():\n",
    "    train_test_preprocessed = pd.read_csv(os.path.join('data', 'train_test_preprocessed.csv'))\n",
    "    train_test_preprocessed_features = train_test_preprocessed.drop('Survived', axis=1)\n",
    "    train_test_preprocessed_label = train_test_preprocessed['Survived']\n",
    "    return train_test_preprocessed_features, train_test_preprocessed_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\dev\\projects\\kaggle-titanic\\wandb\\run-20230924_081838-8abnco4k</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/8abnco4k' target=\"_blank\">olive-firebrand-5</a></strong> to <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/8abnco4k' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/8abnco4k</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▇█▅█▆▃▅▆▂▄▅▃▅▁▄▃▅▂▅▄▄▃▄▃▃▂▅▆▂▃▅▆▄▂▅▅▅▁▃▃</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10000</td></tr><tr><td>loss</td><td>0.32612</td></tr><tr><td>test_accuracy</td><td>0.73119</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">olive-firebrand-5</strong> at: <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/8abnco4k' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/8abnco4k</a><br/> View job at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v1' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v1</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230924_081838-8abnco4k\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888884685, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0971584ea65641bc9cdae38bebf25f82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\dev\\projects\\kaggle-titanic\\wandb\\run-20230924_082112-4om257q0</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/4om257q0' target=\"_blank\">dainty-mountain-6</a></strong> to <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/4om257q0' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/4om257q0</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▄▆▃▇▄▅▅▅▄▇▅▄▄▄▂▅▄█▆▂▃█▄▄▃▃▃▁▄▃▆▆▄▃▅▆▃▇▂▄</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10000</td></tr><tr><td>loss</td><td>0.2434</td></tr><tr><td>test_accuracy</td><td>0.75663</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">dainty-mountain-6</strong> at: <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/4om257q0' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/4om257q0</a><br/> View job at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v2' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v2</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230924_082112-4om257q0\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ef13611bcdb42ae9e2fc06fe54307a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\dev\\projects\\kaggle-titanic\\wandb\\run-20230924_082305-iqerk64j</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/iqerk64j' target=\"_blank\">ruby-jazz-7</a></strong> to <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/iqerk64j' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/iqerk64j</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▆▇▆▇▅▅▃▂▄▅▅▃▄▅▄▃▅▅▅▅▆▅▅▄█▅▄▅▆▄▃▄▅▆▅▆▅▅▂▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10000</td></tr><tr><td>loss</td><td>0.28163</td></tr><tr><td>test_accuracy</td><td>0.74395</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">ruby-jazz-7</strong> at: <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/iqerk64j' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/iqerk64j</a><br/> View job at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v3' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v3</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230924_082305-iqerk64j\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b9f385982724b56b0f4ede994e023e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\dev\\projects\\kaggle-titanic\\wandb\\run-20230924_082450-v5ezs9vp</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/v5ezs9vp' target=\"_blank\">dashing-snowball-8</a></strong> to <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/v5ezs9vp' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/v5ezs9vp</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>███▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10000</td></tr><tr><td>loss</td><td>0.21418</td></tr><tr><td>test_accuracy</td><td>0.75144</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">dashing-snowball-8</strong> at: <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/v5ezs9vp' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/v5ezs9vp</a><br/> View job at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v4' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v4</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230924_082450-v5ezs9vp\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1abf8a498dc941c68fcc10f9f8ab0c1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\dev\\projects\\kaggle-titanic\\wandb\\run-20230924_082619-w2xt61g7</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/w2xt61g7' target=\"_blank\">dainty-sun-9</a></strong> to <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/w2xt61g7' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/w2xt61g7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>███▇▇▇▇▆█▆▇▆▆▅▅▅▅▇▅▅▄▄▄▃▃▄▄▃▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10000</td></tr><tr><td>loss</td><td>0.2102</td></tr><tr><td>test_accuracy</td><td>0.76832</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">dainty-sun-9</strong> at: <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/runs/w2xt61g7' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/runs/w2xt61g7</a><br/> View job at <a href='https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v5' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-titanic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDM5NjcxNg==/version_details/v5</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230924_082619-w2xt61g7\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(7, 12):  # 2^7 = 128, 2^12 = 4096\n",
    "    # Initialize wandb\n",
    "    # config hyperparameter (usually from command line) and let W&B know about them\n",
    "    config = argparse.Namespace()\n",
    "    config.learning_rate = 1e-3\n",
    "    config.epochs = 10000\n",
    "    config.batch_size = 2**i\n",
    "\n",
    "    wandb.init(project=\"kaggle-titanic\", config=vars(config))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    train_features, train_labels = load_titanic_train_preprocessed()\n",
    "\n",
    "    # convert to PyTorch tensors\n",
    "    features_tensors = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "    targets_tensors = torch.tensor(train_labels.values, dtype=torch.float32)\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(features_tensors, targets_tensors)\n",
    "\n",
    "    # Define data loader\n",
    "    # It contains both the features and the labels\n",
    "    batch_size = config.batch_size\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    wandb.watch(model)\n",
    "    model.train(mode=True)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(config.epochs):\n",
    "        for batch_inputs, batch_targets in data_loader:\n",
    "            # batch_inputs --> my features\n",
    "            # batch_targets --> my expected labels\n",
    "\n",
    "            # Clear gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # now we make predictions from our model\n",
    "            batch_outputs = model(batch_inputs.to(device))\n",
    "\n",
    "            # and we calculate the loss by comparing the predictions against our expected labels\n",
    "            # since batch_targets has shape [32] (1 row, 32 cols) and batch_outputs has [32,1]\n",
    "            # (32 rows, 1 col), we do an unsqueeze here to change the shape of batch_targets\n",
    "            loss = loss_function(batch_outputs, batch_targets.unsqueeze(1).to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Backpropagate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        wandb.log({'epoch': epoch+1, 'loss': loss.item()})\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_features, test_labels = load_titanic_train_test_preprocessed()\n",
    "\n",
    "    # convert to PyTorch tensors\n",
    "    test_features_tensors = torch.tensor(test_features.values, dtype=torch.float32)\n",
    "\n",
    "    predicted_labels = model(test_features_tensors.to(device))\n",
    "\n",
    "    # The predicted_labels contain values between 0 and 1.\n",
    "    # We apply a threshold to make a binary classification.\n",
    "    threshold = 0.5\n",
    "    binary_predictions = [1 if prob >= threshold else 0 for prob in predicted_labels]\n",
    "\n",
    "    wandb.log({\"test_accuracy\": f1_score(test_labels, binary_predictions, average=\"weighted\")})\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T06:27:53.011003800Z",
     "start_time": "2023-09-24T06:18:38.068064700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
